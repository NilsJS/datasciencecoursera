---
title: "Regression Models - Course Project"
output:
  pdf_document:
    fig_caption: yes
  html_document: default
includes:
  in_header: mystyles.sty
---

```{r, results="hide", echo=FALSE}
# Load libraries
library(dplyr, quietly=TRUE, warn.conflicts = FALSE)
suppressWarnings(library(scales, quietly=TRUE))
suppressWarnings(library(pander, quietly=TRUE))
library(ggplot2)
suppressWarnings(library(gridExtra, quietly=TRUE, warn.conflict=FALSE))

library(datasets)
library(car)
```


```{r echo = FALSE}
as.gpm <- function (mpg) { 100 / mpg }
as.mpg <- function (gpm) { 1 / (gpm/100) }

data(mtcars)
mtcars <- subset(mtcars, select=c(mpg, am, wt, cyl, disp, hp, drat, qsec, vs, am, gear, carb))

mtcars2 <- subset(mtcars, select=c(am, wt, cyl, disp, hp, drat, qsec, vs, am, gear, carb))
mtcars2$gpm <- as.gpm(mtcars$mpg)

automatic <- subset(mtcars, am == 1)
manual <- subset(mtcars, am == 0)
```

# Executive summary
Using the mtcars data set I have modeled the impact of automatic vs a manual transmission on MPG.  The best model has **Weight** (wt) and **Engine Power** (hp) along with **Transmission Type** (am).  However there is much 

* Is an automatic or manual transmission better for MPG?  
    We are not able to tell from this data set; p-valkues too high for us to reject $H_0$
* Quantify the MPG difference between automatic and manual transmissions? 
    -0.743 MPG with a confidence interval of (2.4965 ; -3.9828)

# Data exploration

```{r echo = FALSE}
# Get he best and the worst by mpg
panderOptions('table.split.table', Inf) 
pander(rbind(head(mtcars[order(mtcars$mpg, decreasing = TRUE),], 2),
             head(mtcars[order(mtcars$mpg, decreasing = FALSE),], 2)), 
       style="rmarkdown", caption="Best and worst cars by MPG")

# Summarize by transmission type
panderOptions('table.split.table', Inf) 
pander(summarise(group_by(mtcars, am), 
                 N=length(am), 
                 mean(mpg), 
                 min(mpg), 
                 max(mpg), 
                 sd(mpg)), 
       style="rmarkdown", caption="MPG by transmission type")
```

The mtcars data set comprises fuel consumption (mpg) and 10 aspects of car performance and design.  Table 1 lists the cars with the highest and the lowest MPG in the set.  Each variable impacts mpg in some way, and transmission type (am) is just one of them.  The scatter plots in Figure \ref{fig:explore} explore three likely predictors of low MPG: *hp*, *wt*, and *disp*. Each colored according transmission type.  All three  have a negative impact on MPG, and they do tend to go together; Large, heavy cars typically have more powerful engines and ultimately consume more fuel, and these cars are more likely to have a manual transmission.  Smaller, lighter and less powerful cars with automatic transmission seem to cluster in the upper left corner, and are more fuel efficient than heavy and/or powerful manual cars which seem overly represented in the lower right corner  

# Models

```{r echo = FALSE}

fit_mpg <- lm(mpg ~ ., mtcars)
coef_mpg <- summary(fit_mpg)$coef
```

The "holistic" model, **mpg ~ .**, takes all variables into consideration.  Table 3 shows the coefficients for this model.  It predicts that switching to automatic transmission will *improve* MPG by `r round(coef_mpg[2,1], 3)`, provided all other factors are constant (basically swapping transmission type in the same car).  In comparison, adding 1000lb of weight to the car will *decrease* MPG by `r abs(round(coef_mpg[3,1], 3))`.  Again providing all other variables stay constant.  However including this many factors have inflated the standard error.  Thus the test for $H_0:\beta_{am} = 0$ vs $H_a:\beta_{am} \ne 0$ isn't significant since `r round(coef_mpg[2,4], 3)` is more than the typical limit of 0.05.  In fact none of the listed p-values are below 0.05.  We have to focus our model on just the most likely contributor to fuel efficiency.

```{r echo = FALSE}
panderOptions('table.split.table', Inf) 
pander(coef_mpg, style="rmarkdown", caption="mpg ~ .")
```

## Model selection. 

We know weight is important, but what else?  Try creating a model for each of the variables along with am:

```{r echo = TRUE}
m1 <- lm(mpg ~ factor(am), data=mtcars)
m2 <- lm(mpg ~ factor(am) * wt, data=mtcars)
m3 <- lm(mpg ~ factor(am) * (wt + hp), data=mtcars)
m4 <- lm(mpg ~ factor(am) * (wt + hp + disp), data=mtcars)
m5 <- lm(mpg ~ factor(am) * (wt + hp + disp + gear), data=mtcars)
m6 <- lm(mpg ~ factor(am) * (wt + hp + disp + gear + carb), data=mtcars)
m7 <- lm(mpg ~ factor(am) * (wt + hp + disp + gear + carb + cyl), data=mtcars)
m8 <- lm(mpg ~ factor(am) * (wt + hp + disp + gear + carb + cyl + drat), data=mtcars)
m9 <- lm(mpg ~ factor(am) * (wt + hp + disp + gear + carb + cyl + drat + qsec), data=mtcars)
m10 <- lm(mpg ~ factor(am) * (wt + hp + disp + gear + carb + cyl + drat + qsec + vs), data=mtcars)
```

```{r echo = FALSE}
a <- anova(m1, m2, m3, m4, m5, m6, m7, m8, m9, m10)
panderOptions('table.split.table', Inf) 
pander(a, style="rmarkdown", caption="anova(m1, m2, m3, m4, m5, m6, m7, m8, m9, m10)")
```

It seems from *anova* that models 2, 3 and 9, were significant, wt, hp & qsec.  Try again with just those:

```{r echo = TRUE}
m11 <- lm(mpg ~ factor(am) * (wt + hp + qsec), data=mtcars)
```

```{r echo = FALSE}
a <- anova(m1, m2, m3, m11)
panderOptions('table.split.table', Inf) 
pander(a, style="rmarkdown", caption="anova(m1, m2, m3, m11)")
```

All models get an acceptable p-value.  Let us see the impact on am as predictor in these models:

```{r echo = FALSE}
cmp <- rbind(summary(m1)$coef[2,],
             summary(m2)$coef[2,],
             summary(m3)$coef[2,],
             summary(m11)$coef[2,])

panderOptions('table.split.table', Inf) 
pander(cmp, style="rmarkdown", caption="Impact on the am predictor for m1, m2, m3 & m11")
```

Including qsec greatly increases the standard error and we no longer get a significant result for am.  **Let's go with m3 as the model.**  In terms of *variance inflation* transmission has a large overall impact, and it also impacts weight and engine power. 

```{r echo = FALSE}
panderOptions('table.split.table', Inf) 
pander(as.table(vif(m3)), style="rmarkdown", caption="Variance Inflation Factors: vif(m3)")
```

## Final model: mpg ~ am * (wt + hp)

Our predictors will be weight (wt) and engine power (hp), along with transmission type (am).  We will center wt and hp to facilitate the interpretation of the coefficients.

```{r echo = TRUE}
mtcars$mwt = mtcars$wt - mean(mtcars$wt)
mtcars$mhp = mtcars$hp - mean(mtcars$hp)

m <- lm(mpg ~ factor(am) * (mwt + mhp), data=mtcars)
```

```{r echo = FALSE}
coef_m <- summary(m)$coef

# Confidence interval for the intercept and the slope
am_ci <- coef_m[2,1] + c(1,-1)*qt(.975, df=m$df) * coef_m[2,2]
wt_ci <- coef_m[3,1] + c(1,-1)*qt(.975, df=m$df) * coef_m[3,2]
hp_ci <- coef_m[4,1] + c(1,-1)*qt(.975, df=m$df) * coef_m[4,2]


panderOptions('table.split.table', Inf) 
pander(coef_m, style="rmarkdown", caption="mpg ~ factor(am) * (mwt + mhp))")
```

Figure \ref{fig:mod_m} makes a plot of this model, which uses weight and horse power along with transmission type as predictors.  For weight in particular there is a clear curvature. This is also apparent in the exploratory plots.  The p-value for transmission type (am), `r coef_m[2,4]`, is far above below the 0.05 limit. Therefore the test for $H_0:\beta_{am} = 0$ vs $H_a:\beta_{am} \ne 0$ isn't significant, and **we cannot conclude that transmission type is a good predictor**. This is possibly due to the lack of overlap between the cars in the two transmission type sets.  With both plots you can also see some outlier values on the right hand side of the plot.  In the residuals plot in Figure \ref{fig:resid_m} you will also see that there are outliers among the lighter cars such as the Fiat 128, and Toyota Corolla.  A linear model is probably not the best fit here.

A car with manual transmission and medium weight(`r round(mean(manual$wt)*1000, 1)` lb), and medium engine power(`r round(mean(manual$hp), 1)` hp) runs `r round(coef_m[1,1],3)` MPG. Switching to automatic transmission (with the same wt and hp) will negatively impact MPG by `r round(coef_m[2,1], 3)` to `r round(coef_m[1,1] + coef_m[2,1],3)` MPG.  Following the wt graph in Figure \ref{fig:mod_m} we see that the slope for automatic transmission is much steeper.  I.e adding 1000lb of weight to a car with manual transmission will cost you less MPG than with an automatic car. The wt slope is the impact of adding 1000lb of weight to a car, while keeping hp and transmission type constant.  For a car with manual transmission this is `r coef_m[3,1]`, while the wt slope for an automatic car is `r coef_m[3,1]` + `r coef_m[5,1]` = `r coef_m[3,1] + coef_m[3,1]`.

The confidence interval for the slope of the *am* predictor/coefficient (`r round(am_ci[1], 4)` ; `r round(am_ci[2], 4)`) is quite wide and it does include 0, so it is quite possible that there is NO effect.  The residual variance `r round(var(resid(m)), 4)` ($\sigma$ = `r round(summary(m)$sigma, 4)`), and the residual plots in Figure \ref{fig:resid_m} shows a couple of strong outliers both in terms of light underpowered cars and some very heavy or overpowered cars.  All this results in a bad fit for am as a predictor in this model. 

# Appendix : plots

```{r explore, echo = FALSE, fig.height = 3, fig.cap="\\label{fig:explore}Exploratory plots of Automatic vs Manual and MPG %>% "}
g1 = ggplot(mtcars, aes(x=hp, y=mpg, color = factor(am))) +
    geom_point(alpha=.5) + 
    theme(legend.position="bottom")

g2 = ggplot(mtcars, aes(x=wt, y=mpg, color = factor(am))) +
    geom_point(alpha=.5) + 
    theme(legend.position="bottom")

g3 = ggplot(mtcars, aes(x=disp, y=mpg, color = factor(am))) +
    geom_point(alpha=.5) + 
    theme(legend.position="bottom")

grid.arrange(g1, g2, g3, ncol=3)
```


```{r mod_m, echo = FALSE, fig.height = 6, fig.width=automatic, fig.cap="\\label{fig:mod_m} mpg ~ am * (mwt + mhp)"}

#automatic <- subset(mtcars, am == 1)
#manual <- subset(mtcars, am == 0)

g1 <- ggplot(mtcars, aes(x=mwt, y=mpg, color=factor(am), size = mhp)) +
        geom_point(alpha=.5) +
        geom_smooth(method = "lm", se = TRUE) +
#        geom_hline(yintercept = mean(automatic$mpg), color = "cyan") +
#        geom_hline(yintercept = mean(manual$mpg), color = "salmon") +
        theme(legend.position="right")
g2 <- ggplot(mtcars, aes(x=mhp, y=mpg, color=factor(am), size = mwt)) +
        geom_point(alpha=.5) +
        geom_smooth(method = "lm", se = TRUE) +
#        geom_hline(yintercept = mean(automatic$mpg), color = "cyan") +
#        geom_hline(yintercept = mean(manual$mpg), color = "salmon") +
        theme(legend.position="right")
grid.arrange(g1, g2, nrow=2)
```


```{r mod_m3_diagnostic, echo = FALSE, fig.height = 6, fig.width=automatic, fig.cap="\\label{fig:mod_m3_diagnostic} Diagnostics for mpg ~ am * (wt + hp)"}
# diagnostic plots 
 layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page 
 plot(m3)
```

```{r resid_m, echo = FALSE, fig.height = 4, fig.width=automatic, fig.cap="\\label{fig:resid_m}Residual plots"}
mtcars$Residual <- resid(m) 
g1 <- ggplot(mtcars, aes(x=wt, y=Residual, color=factor(am), size = hp)) +
      geom_point(alpha=.5) + 
      geom_hline(yintercept = 0, color = "black") +
      theme(legend.position="right")
g2 <- ggplot(mtcars, aes(x=hp, y=Residual, color=factor(am), size = wt)) +
      geom_point(alpha=.5) + 
      geom_hline(yintercept = 0, color = "black") +
      theme(legend.position="right")
grid.arrange(g1, g2, nrow=2)
```


