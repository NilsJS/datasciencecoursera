---
title: "Machine Learning Project - Human Activity Recognition "
includes:
  in_header: mystyles.sty
output:
  html_document: default
  pdf_document:
    fig_caption: yes
bibliography: bibliography.bib
---

```{r, results="hide", echo=FALSE}
# Load libraries
library(dplyr, quietly=TRUE, warn.conflicts = FALSE)
suppressWarnings(library(scales, quietly=TRUE))
suppressWarnings(library(pander, quietly=TRUE))
library(ggplot2)
suppressWarnings(library(gridExtra, quietly=TRUE, warn.conflict=FALSE))
library(lattice)
library(caret)
library(AppliedPredictiveModeling)
suppressPackageStartupMessages(library(randomForest, quietly=TRUE, warn.conflict=FALSE))
library(colorspace)
library(grid)
library(data.table, quietly=TRUE, warn.conflict=FALSE)
suppressPackageStartupMessages(library(VIM, quietly=TRUE, warn.conflict=FALSE))
library(curl)
suppressPackageStartupMessages(library(rattle))
suppressPackageStartupMessages(library(rpart.plot))
```

# Overview 

The data is based on the HAR study [@har2012], where six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (**Class A**), throwing the elbows to the front (**Class B**), lifting the dumbbell only halfway (**Class C**), lowering the dumbbell only halfway (**Class D**) and throwing the hips to the front (**Class E**).  More information is available from the website here: http://groupware.les.inf.puc-rio.br/har

I will use data from accelerometers on the **belt**, **forearm**, **arm**, and **dumbell** of these participants.  The goal of this project is to predict the manner in which they did the exercise.  This is the **"classe"** variable in the training set.

# Data aquisition and cleanup

```{r download_and_read, cache=TRUE, echo = TRUE}
# Download the test and training data sets
if (!file.exists("./data")) dir.create("./data")

# Training data
tdf <- "./data/pml-training.csv"
if (!file.exists(tdf))
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                  destfile=tdf, method="curl")
# Treat the division by zero erros as NA
traindat <- read.csv(tdf, na.strings = c("NA", "#DIV/0!", "", "NaN"))

# Only use the variables associated with the focus areas: belt, forearm, arm, and dumbbell:
traindat <- select(traindat,
                   new_window,
                   classe, 
                   contains('_belt'), 
                   contains('_forearm'), 
                   contains('_arm'), 
                   contains('_dumbbell'))
# Testing data
tstf <- "./data/pml-testing.csv"
if (!file.exists(tstf))
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
                destfile=tstf, method="curl")
testdat <- read.csv(tstf, na.strings = c("NA", "#DIV/0!", "", "NaN"))
testdat <- select(testdat,
                  new_window,
                  contains('_belt'), 
                  contains('_forearm'), 
                  contains('_arm'), 
                  contains('_dumbbell'))
```

I'm treating *division by zero* errors, empty string, "NaN" and "NA", as *"Not Available"* - **NA**.  

# Models - new_window == yes/no

From exploring the data it seems that unless new_window is yes, all the aggregated variables (avg, var, sdtdev, max, min) are empty.  I will try two sets based on wether new_window is 'yes' or 'no'.

```{r create_new_window_sets}
traindat1 <- subset(traindat, new_window == 'yes')
testdat1 <- subset(testdat, new_window == 'yes')
traindat2 <- subset(traindat, new_window == 'no')
testdat2 <- subset(testdat, new_window == 'no')
```

## Set1: Investigate zero variance
```{r nsv_traindat1}
nsv <- nearZeroVar(traindat1, saveMetrics=TRUE)
subset(nsv, zeroVar==TRUE)
```

The kurtosis, skewness, and amplitude variables have zero variance. I will remove these. 

```{r remove_nsv_training1}
# Remove columns that near zero variance
training1 <- select(traindat1, -one_of(rownames(subset(nsv, zeroVar==TRUE))))
testing1 <- select(testdat1, -one_of(rownames(subset(nsv, zeroVar==TRUE))))
```

## Set2: Investigate zero variance

```{r nsv_traindat2}
nsv <- nearZeroVar(traindat2, saveMetrics=TRUE)
head(subset(nsv, zeroVar==TRUE), 20)
```

In the traindat2 set (newq_window='no') the aggregated variables are empty.  I will remove these.

```{r remove_nsv_traindat2}
training2 <- select(traindat2, -one_of(rownames(subset(nsv, zeroVar==TRUE))))
testing2 <- select(testdat2, -one_of(rownames(subset(nsv, zeroVar==TRUE))))
```

## Data partition
Partition the data sets so that we have some test data to work on before going for the official test set.  The test set we downloaded is just for final validation, it does not have the classe variable, so we need to test our model on a subset of the training data. 

```{r data_partitions}
set.seed(198346)
# Set 1
inTrain1 <- createDataPartition(y=training1$classe, p=0.75, list=FALSE)
train1 <- training1[inTrain1, ]
preTest1 <- training1[-inTrain1, ]
# Set 2
inTrain2 <- createDataPartition(y=training2$classe, p=0.75, list=FALSE)
train2 <- training2[inTrain2, ]
preTest2 <- training2[-inTrain2, ]
```

## Cross validation

Use cross validation in order to reduce bias and/or variance.
```{r create_cvs}
control1 <- trainControl(method="cv", number=4, allowParallel = TRUE) 
control2 <- trainControl(method="cv", number=7, allowParallel = TRUE) 
```


## Build models (fitting)

I have chosen the random forest algorithm as it is pretty good in classification, but in order to avoid over fitting I'm using cross validation.  The first model will try 4-fold cross validation.

### Random forest model with 4-fold cross validation (data set 1 - new_window=yes)

For model1 I will impute data for the missing values (NAs).

```{r train_model1, echo=TRUE, warning=FALSE, cache=TRUE, message=FALSE}
set.seed(239045)
modelFit1 <- train(classe~., data=train1, trControl=control1,
                   preProcess=c("knnImpute"), # Use K-Nearest-Neighbor to impute values for all the NAs
                   prox=TRUE,
                   method="rf") # use random forest 
modelFit1
```

The results from this first model are not very good.  Accuracy is in the 70-80% range.  

### Random forest model with 4-fold cross validation (data set 2 - new_window=no)

The training2 set is very large, so to save time I have chosen to limit the number of trees to 100.

```{r train_model2, echo=TRUE, warning=FALSE, cache=TRUE, message=FALSE}
set.seed(239045)
modelFit2 <- train(classe~., data=train2, trControl=control1,
                   na.action = na.omit,
                   method="rf", # use random forest 
                   prox=TRUE,
                   ntree=100)
modelFit2
```

Accuracy is much better for modelFit2, it seems that data set 2 (new_window=yes) is the better approach.  

### Random forest model with 7-fold cross validation (data set 2 - new_window=no)

Let us see if 7-fold cross validation is better than 4-fold...

```{r train_model3, echo=TRUE, warning=FALSE, cache=TRUE, message=FALSE}
set.seed(239045)
modelFit3 <- train(classe~., data=train2, trControl=control2,
                   na.action = na.omit,
                   method="rf", # use random forest 
                   prox=TRUE,
                   ntree=100)
modelFit3
```

7-fold cross validation did the trick.  Accuracy is better than for 4-fold. 

### rpart model (data set 2 - new_window=no)

Let us compare the random forest model with an rpart version...

```{r train_rpart_model, echo=TRUE, warning=FALSE, cache=TRUE, message=FALSE}
set.seed(239045)
modelFitRpart <- train(classe~., data=train2, method="rpart")
modelFitRpart
```

Rpart is nice in the sense that it is easier to visualize and understand.

```{r plot_rpart}
fancyRpartPlot(modelFitRpart$finalModel)
```

Nice as this model is, the random Forest model is more accurate.  

## Final Model

We will go with **modelFit3** (random forest with 7-fold cross validation).

```{r print_final_model}
modelFit3$finalModel
summary(modelFit3$finalModel)
head(getTree(modelFit2$finalModel, k=27), 20) # 27 was the final model chosen
```

# Prediction on pre-test sets
```{r prediction, echo=TRUE, cache=TRUE}
predTest <- predict(modelFit3, newdata = preTest2)
confmat <- confusionMatrix(preTest2$classe, predTest)
confmat
```

Accuracy for modelFit3:  **`r percent(confmat$overall[1])`**

## Predicted *out of sample* error

What error rate can we expect when we apply this model to a new data set, such as the official test for this assignment?  Out of sample error is a problem that may be caused by overfitting.  

```{r}
ose <- 1 - confmat$overall[1]
```

Estimated out of sample error for modelFit3 is *1 - Accuracy* for the final model above: 1 - `r round(confmat$overall[1],4)` = **`r ose`** (`r percent(ose)`).  I.e we expect `r round(1000*ose, 0)` errors out of every 1000 predictions on a new test set. 

# Conclusion
In pre processing I have filtered the data set on **new_window=no**.  I have removed variables with **zero variance**.  The most accurate model I have found is **random forest** with **7-fold cross validation**.  Given a predicted **`r percent(1 - confmat$overall[1])`** out of sample errors, I can expect to get `r round(ose*20, 2)` predictions wrong in the 20 final test cases.  As this is less than 1 the model should correctly predict all cases.

# Prediction and Submission
Run prediction on official test set (20 different test cases). This set does not have the *classe* variable.
```{r official_prediction}
predicted_answers <- predict(modelFit2, testing2)
predicted_answers
```

```{r write_submission_files, echo = FALSE}

# Creating files for submission
write_submission_files = function(x){
     n = length(x)
     for(i in 1:n){
          write.table(x[i],
                      file=paste0("problem_id_",i,".txt"),
                      quote=FALSE,row.names=FALSE,col.names=FALSE)
     }
}

write_submission_files(predicted_answers)
```


# Bibliography
