---
title: "Machine Learning Project - Human Activity Recognition "
includes:
  in_header: mystyles.sty
output:
  html_document: default
  pdf_document:
    fig_caption: yes
bibliography: bibliography.bib
---

```{r, results="hide", echo=FALSE}
# Load libraries
library(dplyr, quietly=TRUE, warn.conflicts = FALSE)
suppressWarnings(library(scales, quietly=TRUE))
suppressWarnings(library(pander, quietly=TRUE))
library(ggplot2)
suppressWarnings(library(gridExtra, quietly=TRUE, warn.conflict=FALSE))
library(lattice)
library(caret)
library(AppliedPredictiveModeling)
suppressPackageStartupMessages(library(randomForest, quietly=TRUE, warn.conflict=FALSE))
library(colorspace)
library(grid)
library(data.table, quietly=TRUE, warn.conflict=FALSE)
suppressPackageStartupMessages(library(VIM, quietly=TRUE, warn.conflict=FALSE))
```

# Assignment Background
The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing **how you built your model**, how you used **cross validation**, what you think the **expected out of sample error** is, and why you made the **choices** you did. You will also use your prediction model to **predict 20 different test cases**. 

1. Your submission should consist of a link to a Github repo with your R markdown and compiled HTML file describing your analysis. Please constrain the text of the write up to < 2000 words and the number of figures to be less than 5. It will make it easier for the graders if you submit a repo with a gh-pages branch so the HTML page can be viewed online (and you always want to make it easy on graders :-).

2. You should also apply your machine learning algorithm to the 20 test cases available in the test data above. Please submit your predictions in appropriate format to the programming assignment for automated grading. See the programming assignment for additional details. 


# Overview 
The data is based on the HAR study [@har2012], where six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (**Class A**), throwing the elbows to the front (**Class B**), lifting the dumbbell only halfway (**Class C**), lowering the dumbbell only halfway (**Class D**) and throwing the hips to the front (**Class E**).  More information is available from the website here: http://groupware.les.inf.puc-rio.br/har

I will use data from accelerometers on the **belt**, **forearm**, **arm**, and **dumbell** of these participants.  The goal of this project is to predict the manner in which they did the exercise.  This is the **"classe"** variable in the training set.

## Data aquisition and cleanup
```{r cache=TRUE, echo = TRUE}
# Download the test and training data sets
if (!file.exists("./data")) dir.create("./data")

# Training data
tdf <- "./data/pml-training.csv"
if (!file.exists(tdf))
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                  destfile=tdf, method="curl")
# Treat the division by zero erros as NA
traindat <- read.csv(tdf, na.strings = c("NA", "#DIV/0!", "", "NaN"))

# Only use the variables associated with the focus areas: belt, forearm, arm, and dumbbell:
traindat <- select(traindat,
                   new_window,
                   classe, 
                   contains('_belt'), 
                   contains('_forearm'), 
                   contains('_arm'), 
                   contains('_dumbbell'))
# Testing data
tstf <- "./data/pml-testing.csv"
if (!file.exists(tstf))
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
                destfile=tstf, method="curl")
testdat <- read.csv(tstf, na.strings = c("NA", "#DIV/0!", "", "NaN"))
testdat <- select(testdat,
                  new_window,
                  contains('_belt'), 
                  contains('_forearm'), 
                  contains('_arm'), 
                  contains('_dumbbell'))
```

I'm treating *division by zero* errors, empty string, "NaN" and "NA", as *"Not Available"* - **NA**.  

# Models - new_window == yes/no

From exploring the data it seems that unless new_window is yes, all the aggregated variables (avg, var, sdtdev, max, min) are empty.  I will try two sets based on wether new_window is 'yes' or 'no'.

```{r}
traindat1 <- subset(traindat, new_window == 'yes')
testdat1 <- subset(testdat, new_window == 'yes')
traindat2 <- subset(traindat, new_window == 'no')
testdat2 <- subset(testdat, new_window == 'no')
```

## Set1: Investigate zero variance
```{r}
nsv <- nearZeroVar(traindat1, saveMetrics=TRUE)
subset(nsv, zeroVar==TRUE)
```

The kurtosis, skewness, and amplitude variables have zero variance. I will remove these. 

```{r}
# Remove columns that near zero variance
training1 <- select(traindat1, -one_of(rownames(subset(nsv, zeroVar==TRUE))))
testing1 <- select(testdat1, -one_of(rownames(subset(nsv, zeroVar==TRUE))))
```

## Set2: Investigate zero variance

```{r}
nsv <- nearZeroVar(traindat2, saveMetrics=TRUE)
head(subset(nsv, zeroVar==TRUE), 20)
```

In the traindat2 set (newq_window='no') the aggregated variables are empty.  I will remove these.

```{r}
training2 <- select(traindat2, -one_of(rownames(subset(nsv, zeroVar==TRUE))))
testing2 <- select(testdat2, -one_of(rownames(subset(nsv, zeroVar==TRUE))))
```

## Data partition
Partition the data sets so that we have some testdata to work on before going for the official test set.  The test set we downloaded is just for final validation, it does not have the classe variable, so we need to test our model on a subset of the training data. 

```{r}
set.seed(198346)
# Set 1
inTrain1 <- createDataPartition(y=training1$classe, p=0.75, list=FALSE)
train1 <- training1[inTrain1, ]
preTest1 <- training1[-inTrain1, ]
# Set 2
inTrain2 <- createDataPartition(y=training2$classe, p=0.75, list=FALSE)
train2 <- training2[inTrain2, ]
preTest2 <- training2[-inTrain2, ]
```

## Cross validation

Use cross validation in order to reduce bias and/or variance.
```{r}
control1 <- trainControl(method="cv", number=4, allowParallel = TRUE) 
control2 <- trainControl(method="cv", number=7, allowParallel = TRUE) 
```


## Build models (fitting)
I have chosen the random forest algorithm as it is pretty good in classification, but in order to avoid overfitting I'm using cross validation.  THe first model will try 4-fold cross validation.

For model1 I will impute data for the missing values (NAs).

```{r train_model1, echo=TRUE, warning=FALSE, cache=TRUE, message=FALSE}
set.seed(239045)
modelFit1 <- train(classe~., data=train1, trControl=control1,
                   preProcess=c("knnImpute"), # Use K-Nearest-Neighbor to impute values for all the NAs
                   prox=TRUE,
                   method="rf") # use random forest 
modelFit1
```

The results from this first model are not very good.  Accuracy is in the 70-80% range.  

The training2 set is very large, so to save time I have chosen to limit the number of trees to 100.

```{r train_model2, echo=TRUE, warning=FALSE, cache=TRUE, message=FALSE}
set.seed(239045)
modelFit2 <- train(classe~., data=train2, trControl=control1,
                   na.action = na.omit,
                   method="rf", # use random forest 
                   prox=TRUE,
                   ntree=100)
modelFit2
```

Accuracy is much better for modelFit2.  Let us see if 7-fold cross validation is better than 4-fold...

```{r train_model3, echo=TRUE, warning=FALSE, cache=TRUE, message=FALSE}
set.seed(239045)
modelFit3 <- train(classe~., data=train2, trControl=control2,
                   na.action = na.omit,
                   method="rf", # use random forest 
                   prox=TRUE,
                   ntree=100)
modelFit3
```

### Final Model
```{r}
modelFit2$finalModel

modelFit2$finalModel
head(getTree(modelFit2$finalModel, k=27), 20) # 27 was the final model chosen
#plot(modelFit2$finalModel, 1, pch=19, cex=.5, col="#00000010")
#library(RGtk2)
#library(rattle)
#fancyRpartPlot(modelFit2$finalModel)
```

# Prediction on pre-test sets
```{r prediction, echo=TRUE, cache=TRUE}
predTest <- predict(modelFit2, newdata = preTest2)

confmat <- confusionMatrix(preTest2$classe, predTest)
confmat
```

Accuracy for modelFit2:  `r paste(round(confmat$overall[1]*100,2), "%")`

```{r prediction, echo=TRUE, cache=TRUE}
predTest <- predict(modelFit3, newdata = preTest2)

confmat <- confusionMatrix(preTest2$classe, predTest)
confmat
```

Accuracy for modelFit3:  `r paste(round(confmat$overall[1]*100,2), "%")`

# Receiver Operating Caracteristic (ROC) 


# Accuracy
Run prediction on official test set.  This set does not have the *classe* variable.
```{r}
predTesting2 <- predict(modelFit2, testing2)
predTesting2
#testing2$predRight <- predTesting2$classe
#table(predTesting2, testing2$classe)
#confmat <- confusionMatrix(testing2$classe, predTesting2)
#confmat
#qplot()
```

Expected out of sample error.

# Predictions

Predict 20 different test cases.

# Bibliography
