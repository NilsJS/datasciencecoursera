---
title: "Machine Learning - Quiz 2"
author: "Nils"
date: "16. oktober 2015"
output: html_document
---
```{r, results="hide", echo=FALSE}
# Load libraries
library(dplyr, quietly=TRUE, warn.conflicts = FALSE)
suppressWarnings(library(scales, quietly=TRUE))
suppressWarnings(library(pander, quietly=TRUE))
```

# Question 1

Load the Alzheimer's disease data using the commands: 

```{r}
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
```

Which of the following commands will create training and test sets with about 50% of the observations assigned to each? 

#### Alternative 1
```{r}
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[trainIndex,]
dim(training)
dim(testing)
```

#### Alternative 2
```{r}
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[-testIndex,]
testing = adData[testIndex,]
dim(training)
dim(testing)
```


Alternative **2** is the correct command set.


#### Alternative 3
```{r}
adData = data.frame(predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
dim(training)
dim(testing)
head(trainIndex)
```

#### Alternative 4
```{r}
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[trainIndex,]
dim(training)
dim(testing)
```



# Question 2

Load the cement data using the commands: 

```{r}
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
```

Make a histogram and confirm the SuperPlasticizer variable is skewed. Normally you might use the log transform to try to make the data more symmetric. Why would that be a poor choice for this variable?

Ref: [Log Transformations for Skewed and Wide Distributions](http://www.r-statistics.com/2013/05/log-transformations-for-skewed-and-wide-distributions-from-practical-data-science-with-r/)

```{r}
str(training)
summary(training$Superplasticizer)
hist(training$Superplasticizer)
hist(log(training$Superplasticizer))
hist(log10(training$Superplasticizer))
hist(log10(training$Superplasticizer + 1))
```


1. The log transform does not reduce the skewness of the non-zero values of SuperPlasticizer 
2. The log transform produces negative values which can not be used by some classifiers. 
3. **There are values of zero so when you take the log() transform those values will be -Inf. **
4. The log transform is not a monotone transformation of the data. 

5. The SuperPlasticizer data include negative values so the log transform can not be performed.  

**NO*: There are no negative values in SuperPlasticiser.

4. There are a large number of values that are the same and even if you took the log(SuperPlasticizer + 1) they would still all be identical so the distribution would not be symmetric.

**Yes**: This is also visible in the histograms


# Question 3

Load the Alzheimer's disease data using the commands: 

```{r}
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
```

Find all the predictor variables in the training set that begin with IL. 
```{r}
library(dplyr)
#str(training)
predictors <- select(training, starts_with("IL"))
str(predictors)
```

Perform principal components on these variables with the preProcess() function from the caret package.  Calculate the number of principal components needed to capture 80% of the variance. 

```{r}
# Use the thresh parameter to the pca mathod to indicate variance percentage cutoff
## preProcess function has an argument called thresh that is a threshold for the cumulative percentage of variance captured by the principal components
preProc <- preProcess(predictors, method="pca", thresh =.8)
print(preProc)
```

How many are there? 

1. **7** 
2. 11 
3. 8 
4. 12 


# Question 4

Load the Alzheimer's disease data using the commands: 
```{r}
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
```

Create a training data set consisting of only the predictors with variable names beginning with IL and the diagnosis. 

```{r}
library(dplyr)
#str(training)
predictors <- select(training, diagnosis, starts_with("IL"))
str(predictors)
```

Build two predictive models, one using the predictors as they are and one using PCA with principal components explaining 80% of the variance in the predictors. Use method="glm" in the train function. 

```{r}
# Model 1: Use predictors as they are
m1 <- train(diagnosis~., method="glm", data=predictors)

# Model 2: use PCA with 80% threshold
preProc <- preProcess(predictors, method="pca", thresh =.8)
trainPC <- predict(preProc, predictors)
m2 <- train(diagnosis~., method="glm", data=trainPC)
```

What is the accuracy of each method in the test set? 
```{r}
# Model 1 accuracy
cm1 <- confusionMatrix(testing$diagnosis, predict(m1, newdata=testing))
print(cm1)

# Model 2 accuracy
testPC <- predict(preProc, newdata=testing)
cm2 <- confusionMatrix(testing$diagnosis, predict(m2, newdata=testPC))
print(cm2)
```

Which is more accurate? 

* Non-PCA Accuracy: **`r round(cm1$overall['Accuracy'],2)`**
* PCA Accuracy: **`r round(cm2$overall['Accuracy'],2)`**


#### Alternative 1
* Non-PCA Accuracy: **0.65** 
* PCA Accuracy: **0.72**  

This is the right answer

#### Alternative 2
* Non-PCA Accuracy: 0.72 
* PCA Accuracy: 0.65 

#### Alternative 3
* Non-PCA Accuracy: 0.72 
* PCA Accuracy: 0.71 

#### Alternative 4
* Non-PCA Accuracy: 0.75 
* PCA Accuracy: 0.71